
    
<!DOCTYPE html>
<html lang="en-us">
<meta http-equiv="X-UA-Compatible" content="IE=7" />
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="author" content="Turnitin, LLC" />
    <meta name="keywords" content="" /> 
    <meta name="description" content="" />
    <meta name="viewport" content="width = initial-scale" />
    <meta name="viewport" content="height = initial-scale" />
    
<title>Turnitin Originality Report</title>

<style type="text/css" media="screen">
#actions {
	display: block;
	width: 100%;
	text-align: center;
	border-bottom: 1px solid #888;
	padding: 20px 0px 30px;
	background: #eee;
	}
	
#actions a	{
	color: #00f;
	}

#actions p	{
	text-align: center;
	}
	
</style>

<link rel="stylesheet" type="text/css" href="https://www.turnitin.com/r/build/css/tii/972ffd2859ac15b72890fc369e9e71fecb_newreport_styles_print.css" media="all" />




</head>

<body id="or_print_report">






<div id="top">
    <div id="content">
        <!-- ######### Top Body  ##########################--> 
        <div id="top_body">
            <p id="title">
            <img src="https://www.turnitin.com/r/build/images/b3964fd7e38c8fcbb8bfbf2efc1f0635cb_turnitin_logo.gif" id="logo" width="60">
            Turnitin Originality Report
            </p>
                <div class="general_info">
                    <p>
                        <span>YOLO Sparse Training and Model Pruning for Street View House Numbers Recognition</span>
                        
                        by No No
                        
                    </p>
                    <p>
                        From Check - No Repository (不收录) (Check - No Repository (不收录)	)
                    </p>
                    <ul>
                        <li>Processed on 01-Mar-2023 10:31 PM CST</li>
                        <li>ID: 2026756528</li>
                        <li>Word Count: 3682</li>
                    </ul>
                </div>
                <div class="similarity_box">
                    <div class="overall_similarity">
                        <div class="color_box green">&nbsp;</div>
                        <div class="similarity_title">Similarity Index</div>
                        <div class="similarity_percent">21%</div>
                    </div>
                    <div class="similarity_by_source">
                        <div class="similarity_title">Similarity by Source</div>
                        <dl>
                            <dt>Internet&nbsp;Sources:</dt>
                            <dd>13%</dd>
                            <div class="clear"></div>
                            <dt>Publications:</dt>
                            <dd>19%</dd>
                            <div class="clear"></div>
                            <dt>Student&nbsp;Papers:</dt>
                            <dd>10%</dd>
                            <div class="clear"></div>
                        </dl>
                    </div>
                </div>
                <div class="clear"></div>
                                 
        </div>
        <!-- ######### END Top Body  ##########################--> 
    </div>
</div>

<div class="divider">sources:</div>

<div class="links">

	<div  >
	    
            <div class="number-l">
                1
            </div>
        
	    <p>
            1% match (Internet from 24-Nov-2021) 
        </p>
        
	        <a 
                
                    style="color: #D10A0A" 
                
                href="http://acec.event.upi.edu/file/download/AP_proceedings_template_research_paper.docx"
            >
                http://acec.event.upi.edu/file/download/AP_proceedings_template_research_paper.docx
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                2
            </div>
        
	    <p>
            1% match ("Advances in Artificial Intelligence and Security", Springer Science and Business Media LLC, 2022) 
        </p>
        
	        <a 
                
                    style="color: #287B28" 
                
                href="https://doi.org/10.1007/978-3-031-06767-9"
            >
                "Advances in Artificial Intelligence and Security", Springer Science and Business Media LLC, 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                3
            </div>
        
	    <p>
            1% match (Internet from 05-Jul-2019) 
        </p>
        
	        <a 
                
                    style="color: blue" 
                
                href="https://www.datasetlist.com/"
            >
                https://www.datasetlist.com/
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                4
            </div>
        
	    <p>
            1% match ("Advanced Computer Architecture", Springer Science and Business Media LLC, 2020) 
        </p>
        
	        <a 
                
                    style="color: brown" 
                
                href="https://doi.org/10.1007/978-981-15-8135-9"
            >
                "Advanced Computer Architecture", Springer Science and Business Media LLC, 2020
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                5
            </div>
        
	    <p>
            1% match (Internet from 11-Feb-2023) 
        </p>
        
	        <a 
                
                    style="color: #B64B01" 
                
                href="https://www.researchgate.net/publication/344505619_Comprehensive_Online_Network_Pruning_via_Learnable_Scaling_Factors"
            >
                https://www.researchgate.net/publication/344505619_Comprehensive_Online_Network_Pruning_via_Learnable_Scaling_Factors
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                6
            </div>
        
	    <p>
            1% match (student papers from 02-Jan-2023) 
        </p>
        
	        <a 
                
                    style="color: #630000" 
                
                href="/paperInfo.asp?r=0.236731533477652&svr=55&lang=en_us&oid=oid:2:717586958&n=2&perc=1"
            >
                Submitted to University of Sussex  on 2023-01-02
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                7
            </div>
        
	    <p>
            1% match (Fuxiang Liu, Song He, Zhisheng Chen. "Improved Alexnet-Based Character Detection Method", Journal of Physics: Conference Series, 2022) 
        </p>
        
	        <a 
                
                    style="color: #0270B6" 
                
                href="https://doi.org/10.1088/1742-6596/2278/1/012025"
            >
                Fuxiang Liu, Song He, Zhisheng Chen. "Improved Alexnet-Based Character Detection Method", Journal of Physics: Conference Series, 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                8
            </div>
        
	    <p>
            1% match (Internet from 17-Dec-2022) 
        </p>
        
	        <a 
                
                    style="color: #330099" 
                
                href="https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2625701/no.ntnu%3ainspera%3a2525131.pdf?isAllowed=y&sequence=5"
            >
                https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2625701/no.ntnu%3ainspera%3a2525131.pdf?isAllowed=y&sequence=5
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                9
            </div>
        
	    <p>
            1% match () 
        </p>
        
	        <a 
                
                    style="color: #227967" 
                
                href="https://www.pure.ed.ac.uk/ws/files/19293927/Laminar_and_Dorsoventral_Molecular_Organization_of_the_Medial_Entorhinal_Cortex_Revealed_by_Large_scale_Anatomical_Analysis_of_Gene_Expression.pdf"
            >
                Ramsden, Helen L., Surmeli, Gulsen, McDonagh, Steven G., Nolan, Matthew F.. "Laminar and Dorsoventral Molecular Organization of the Medial Entorhinal Cortex Revealed by Large-scale Anatomical Analysis of Gene Expression", 'Public Library of Science (PLoS)', 2015
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                10
            </div>
        
	    <p>
            < 1% match (Internet from 18-May-2022) 
        </p>
        
	        <a 
                
                    style="color: #CB0099" 
                
                href="https://www.mdpi.com/1424-8220/22/10/3714/htm"
            >
                https://www.mdpi.com/1424-8220/22/10/3714/htm
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                11
            </div>
        
	    <p>
            < 1% match (Internet from 15-Feb-2023) 
        </p>
        
	        <a 
                
                    style="color: #006331" 
                
                href="https://www.mdpi.com/2073-4395/13/2/521"
            >
                https://www.mdpi.com/2073-4395/13/2/521
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                12
            </div>
        
	    <p>
            < 1% match (Zhao Zekuan, He Chunlin. "Research on Defect Detection Method of Drainage Pipe Network Based on Deep Learning", 2022 19th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP), 2022) 
        </p>
        
	        <a 
                
                    style="color: #795AB9" 
                
                href="https://doi.org/10.1109/ICCWAMTIP56608.2022.10016589"
            >
                Zhao Zekuan, He Chunlin. "Research on Defect Detection Method of Drainage Pipe Network Based on Deep Learning", 2022 19th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP), 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                13
            </div>
        
	    <p>
            < 1% match (student papers from 03-Jun-2022) 
        </p>
        
	        <a 
                
                    style="color: #935F32" 
                
                href="/paperInfo.asp?r=0.236731533477652&svr=55&lang=en_us&oid=oid:2:700487233&n=2&perc=0"
            >
                Submitted to University College London  on 2022-06-03
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                14
            </div>
        
	    <p>
            < 1% match (student papers from 30-Sep-2019) 
        </p>
        
	        <a 
                
                    style="color: #ce0031" 
                
                href="/paperInfo.asp?r=0.236731533477652&svr=55&lang=en_us&oid=oid:2:606847673&n=2&perc=0"
            >
                Submitted to University of Derby  on 2019-09-30
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                15
            </div>
        
	    <p>
            < 1% match (Xuan Yang, Shanshan Li, Zhengchao Chen, Jocelyn Chanussot, Xiuping Jia, Bing Zhang, Baipeng Li, Pan Chen. "An attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery", ISPRS Journal of Photogrammetry and Remote Sensing, 2021) 
        </p>
        
	        <a 
                
                    style="color: #866712" 
                
                href="https://doi.org/10.1016/j.isprsjprs.2021.05.004"
            >
                Xuan Yang, Shanshan Li, Zhengchao Chen, Jocelyn Chanussot, Xiuping Jia, Bing Zhang, Baipeng Li, Pan Chen. "An attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery", ISPRS Journal of Photogrammetry and Remote Sensing, 2021
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                16
            </div>
        
	    <p>
            < 1% match (student papers from 01-Nov-2021) 
        </p>
        
	        <a 
                
                    style="color: #63009c" 
                
                href="/paperInfo.asp?r=0.236731533477652&svr=55&lang=en_us&oid=oid:1:2158712838&n=1&perc=0"
            >
                Submitted to University of Melbourne on 2021-11-01
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                17
            </div>
        
	    <p>
            < 1% match (Internet from 01-Nov-2010) 
        </p>
        
	        <a 
                
                    style="color: #A85503" 
                
                href="http://caai.cn:8080/ichs08/files/example-paper-template.doc"
            >
                http://caai.cn:8080/ichs08/files/example-paper-template.doc
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                18
            </div>
        
	    <p>
            < 1% match (Internet from 11-Jan-2023) 
        </p>
        
	        <a 
                
                    style="color: #cc0066" 
                
                href="https://www.frontiersin.org/articles/10.3389/fnbot.2022.881021/full"
            >
                https://www.frontiersin.org/articles/10.3389/fnbot.2022.881021/full
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                19
            </div>
        
	    <p>
            < 1% match (student papers from 29-May-2022) 
        </p>
        
	        <a 
                
                    style="color: #21785B" 
                
                href="/paperInfo.asp?r=0.236731533477652&svr=55&lang=en_us&oid=oid:1:2329125753&n=1&perc=0"
            >
                Submitted to University of Technology, Sydney on 2022-05-29
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                20
            </div>
        
	    <p>
            < 1% match (student papers from 11-Mar-2022) 
        </p>
        
	        <a 
                
                    style="color: #336699" 
                
                href="/paperInfo.asp?r=0.236731533477652&svr=55&lang=en_us&oid=oid:1:2258391872&n=1&perc=0"
            >
                Submitted to University of Wollongong on 2022-03-11
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                21
            </div>
        
	    <p>
            < 1% match (Internet from 29-Sep-2022) 
        </p>
        
	        <a 
                
                    style="color: #D10A0A" 
                
                href="https://downloads.hindawi.com/journals/jhe/2021/6678526.pdf"
            >
                https://downloads.hindawi.com/journals/jhe/2021/6678526.pdf
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                22
            </div>
        
	    <p>
            < 1% match (Internet from 09-Dec-2022) 
        </p>
        
	        <a 
                
                    style="color: #287B28" 
                
                href="http://olarik.it.msu.ac.th/wp-content/uploads/2021/07/Khamket-Surinta2021_Chapter_FeatureExtractionEfficientForF.pdf"
            >
                http://olarik.it.msu.ac.th/wp-content/uploads/2021/07/Khamket-Surinta2021_Chapter_FeatureExtractionEfficientForF.pdf
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                23
            </div>
        
	    <p>
            < 1% match (Internet from 28-Jul-2012) 
        </p>
        
	        <a 
                
                    style="color: blue" 
                
                href="http://www.codeforge.cn/s/0/HAND-GESTURE-RECOGNITION---unity"
            >
                http://www.codeforge.cn/s/0/HAND-GESTURE-RECOGNITION---unity
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                24
            </div>
        
	    <p>
            < 1% match ("Intelligent Computing Theories and Application", Springer Science and Business Media LLC, 2016) 
        </p>
        
	        <a 
                
                    style="color: brown" 
                
                href="https://doi.org/10.1007/978-3-319-42294-7"
            >
                "Intelligent Computing Theories and Application", Springer Science and Business Media LLC, 2016
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                25
            </div>
        
	    <p>
            < 1% match ("Intelligent Equipment, Robots, and Vehicles", Springer Science and Business Media LLC, 2021) 
        </p>
        
	        <a 
                
                    style="color: #B64B01" 
                
                href="https://doi.org/10.1007/978-981-16-7213-2"
            >
                "Intelligent Equipment, Robots, and Vehicles", Springer Science and Business Media LLC, 2021
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                26
            </div>
        
	    <p>
            < 1% match (Jia Zhao. "Design of Intelligent Guidance Algorithm for Language Training in Cross-cultural Communication based on Multicast Multimedia Fusion SSD Framework", 2022 7th International Conference on Communication and Electronics Systems (ICCES), 2022) 
        </p>
        
	        <a 
                
                    style="color: #630000" 
                
                href="https://doi.org/10.1109/ICCES54183.2022.9835973"
            >
                Jia Zhao. "Design of Intelligent Guidance Algorithm for Language Training in Cross-cultural Communication based on Multicast Multimedia Fusion SSD Framework", 2022 7th International Conference on Communication and Electronics Systems (ICCES), 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                27
            </div>
        
	    <p>
            < 1% match (Sirui Shen, Wenqing Song, Xinyu Wang, Xinyu Shao, Yuxiang Fu, Zhonghai Lu, Li Li. "A Hierarchical Parallel Discrete Gaussian Sampler for Lattice-Based Cryptography", 2022 IEEE International Symposium on Circuits and Systems (ISCAS), 2022) 
        </p>
        
	        <a 
                
                    style="color: #0270B6" 
                
                href="https://doi.org/10.1109/ISCAS48785.2022.9937989"
            >
                Sirui Shen, Wenqing Song, Xinyu Wang, Xinyu Shao, Yuxiang Fu, Zhonghai Lu, Li Li. "A Hierarchical Parallel Discrete Gaussian Sampler for Lattice-Based Cryptography", 2022 IEEE International Symposium on Circuits and Systems (ISCAS), 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                28
            </div>
        
	    <p>
            < 1% match (Xingyuan Wang, Xuan Chen. "Image encryption algorithm based on cross-scrambling and rapid-mode diffusion", The Visual Computer, 2022) 
        </p>
        
	        <a 
                
                    style="color: #330099" 
                
                href="https://doi.org/10.1007/s00371-022-02645-5"
            >
                Xingyuan Wang, Xuan Chen. "Image encryption algorithm based on cross-scrambling and rapid-mode diffusion", The Visual Computer, 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                29
            </div>
        
	    <p>
            < 1% match (Internet from 02-Jan-2022) 
        </p>
        
	        <a 
                
                    style="color: #227967" 
                
                href="http://scholar.dkyobobook.co.kr/searchDetail.laf?barcode=4010027334550"
            >
                http://scholar.dkyobobook.co.kr/searchDetail.laf?barcode=4010027334550
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                30
            </div>
        
	    <p>
            < 1% match (Internet from 21-Oct-2022) 
        </p>
        
	        <a 
                
                    style="color: #CB0099" 
                
                href="https://www.hindawi.com/journals/cin/2022/7703444/"
            >
                https://www.hindawi.com/journals/cin/2022/7703444/
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                31
            </div>
        
	    <p>
            < 1% match (Raghunath Dey, Rakesh Chandra Balabantaray. "A Novel Sliding Window Approach for Offline Handwritten Character Recognition", 2019 International Conference on Information Technology (ICIT), 2019) 
        </p>
        
	        <a 
                
                    style="color: #006331" 
                
                href="https://doi.org/10.1109/ICIT48102.2019.00038"
            >
                Raghunath Dey, Rakesh Chandra Balabantaray. "A Novel Sliding Window Approach for Offline Handwritten Character Recognition", 2019 International Conference on Information Technology (ICIT), 2019
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                32
            </div>
        
	    <p>
            < 1% match (Zhen Zhang, Huiying Chen, Ruijie Xiao, Qingjie Li. "Research on Smoking Detection Based on Deep Learning", Journal of Physics: Conference Series, 2021) 
        </p>
        
	        <a 
                
                    style="color: #795AB9" 
                
                href="https://doi.org/10.1088/1742-6596/2024/1/012042"
            >
                Zhen Zhang, Huiying Chen, Ruijie Xiao, Qingjie Li. "Research on Smoking Detection Based on Deep Learning", Journal of Physics: Conference Series, 2021
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                33
            </div>
        
	    <p>
            < 1% match (Internet from 29-Dec-2020) 
        </p>
        
	        <a 
                
                    style="color: #935F32" 
                
                href="http://docplayer.net/50807699-Hybrid-cnn-hmm-model-for-street-view-house-number-recognition.html"
            >
                http://docplayer.net/50807699-Hybrid-cnn-hmm-model-for-street-view-house-number-recognition.html
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                34
            </div>
        
	    <p>
            < 1% match (Internet from 03-May-2014) 
        </p>
        
	        <a 
                
                    style="color: #ce0031" 
                
                href="http://ufldl.stanford.edu/housenumbers/"
            >
                http://ufldl.stanford.edu/housenumbers/
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                35
            </div>
        
	    <p>
            < 1% match (Internet from 31-Dec-2020) 
        </p>
        
	        <a 
                
                    style="color: #866712" 
                
                href="https://www.springerprofessional.de/offline-handwritten-chinese-character-recognition-based-on-new-t/15434992"
            >
                https://www.springerprofessional.de/offline-handwritten-chinese-character-recognition-based-on-new-t/15434992
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                36
            </div>
        
	    <p>
            < 1% match (Advances in Intelligent Systems and Computing, 2015.) 
        </p>
        
	        <a 
                
                    style="color: #63009c" 
                
                href="https://doi.org/10.1007/978-3-319-16841-8"
            >
                Advances in Intelligent Systems and Computing, 2015.
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                37
            </div>
        
	    <p>
            < 1% match (A.G. Hochuli, L.S. Oliveira, A.S. Britto Jr, R. Sabourin. "Handwritten digit segmentation: Is it still necessary?", Pattern Recognition, 2018) 
        </p>
        
	        <a 
                
                    style="color: #A85503" 
                
                href="http://linkinghub.elsevier.com/retrieve/pii/S0031320318300037"
            >
                A.G. Hochuli, L.S. Oliveira, A.S. Britto Jr, R. Sabourin. "Handwritten digit segmentation: Is it still necessary?", Pattern Recognition, 2018
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                38
            </div>
        
	    <p>
            < 1% match (Lichao Liu, Jing Liang, Jianqing Wang, Peiyu Hu, Ling Wan, Quan Zheng. "An improved YOLOv5-based approach to soybean phenotype information perception", Computers and Electrical Engineering, 2023) 
        </p>
        
	        <a 
                
                    style="color: #cc0066" 
                
                href="https://doi.org/10.1016/j.compeleceng.2023.108582"
            >
                Lichao Liu, Jing Liang, Jianqing Wang, Peiyu Hu, Ling Wan, Quan Zheng. "An improved YOLOv5-based approach to soybean phenotype information perception", Computers and Electrical Engineering, 2023
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                39
            </div>
        
	    <p>
            < 1% match (Lin Zhang, Yaqi Sun, Wenli Chen, Xiaoman Liang. "Chapter 24 Traffic Sign Recognition System Based on YOLOv5", Springer Science and Business Media LLC, 2022) 
        </p>
        
	        <a 
                
                    style="color: #21785B" 
                
                href="https://doi.org/10.1007/978-981-19-6901-0_24"
            >
                Lin Zhang, Yaqi Sun, Wenli Chen, Xiaoman Liang. "Chapter 24 Traffic Sign Recognition System Based on YOLOv5", Springer Science and Business Media LLC, 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                40
            </div>
        
	    <p>
            < 1% match (Qingjin Xu, Rui Fu, Fuwei Wu, Biyao Wang. "Roadside estimation of a vehicle’s center of gravity height based on an improved single-stage detection algorithm and regression prediction technology", IEEE Sensors Journal, 2021) 
        </p>
        
	        <a 
                
                    style="color: #336699" 
                
                href="https://doi.org/10.1109/JSEN.2021.3114703"
            >
                Qingjin Xu, Rui Fu, Fuwei Wu, Biyao Wang. "Roadside estimation of a vehicle’s center of gravity height based on an improved single-stage detection algorithm and regression prediction technology", IEEE Sensors Journal, 2021
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                41
            </div>
        
	    <p>
            < 1% match (Wenjing Hong, Guiying Li, Shengcai Liu, Peng Yang, Ke Tang. "Multi-objective evolutionary optimization for hardware-aware neural network pruning", Fundamental Research, 2022) 
        </p>
        
	        <a 
                
                    style="color: #D10A0A" 
                
                href="https://doi.org/10.1016/j.fmre.2022.07.013"
            >
                Wenjing Hong, Guiying Li, Shengcai Liu, Peng Yang, Ke Tang. "Multi-objective evolutionary optimization for hardware-aware neural network pruning", Fundamental Research, 2022
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                42
            </div>
        
	    <p>
            < 1% match (Yunhai Song, Zhenzhen Zhou, Qiang Li, Yizhou Chen, Pengfei Xiang, Qian Yu, Liang Zhang, Yanfeng Lu. "Intrusion detection of foreign objects in high-voltage lines based on YOLOv4", 2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP), 2021) 
        </p>
        
	        <a 
                
                    style="color: #287B28" 
                
                href="https://doi.org/10.1109/ICSP51882.2021.9408753"
            >
                Yunhai Song, Zhenzhen Zhou, Qiang Li, Yizhou Chen, Pengfei Xiang, Qian Yu, Liang Zhang, Yanfeng Lu. "Intrusion detection of foreign objects in high-voltage lines based on YOLOv4", 2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP), 2021
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                43
            </div>
        
	    <p>
            < 1% match (Handbook of Document Image Processing and Recognition, 2014.) 
        </p>
        
	        <a 
                
                    style="color: blue" 
                
                href="https://doi.org/10.1007/978-0-85729-859-1"
            >
                Handbook of Document Image Processing and Recognition, 2014.
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                44
            </div>
        
	    <p>
            < 1% match (Internet from 22-Feb-2023) 
        </p>
        
	        <a 
                
                    style="color: brown" 
                
                href="https://aircconline.com/ijdms/V14N6/14622ijdms01.pdf"
            >
                https://aircconline.com/ijdms/V14N6/14622ijdms01.pdf
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                45
            </div>
        
	    <p>
            < 1% match (Internet from 24-Nov-2022) 
        </p>
        
	        <a 
                
                    style="color: #B64B01" 
                
                href="https://chenkecm.github.io/publications/"
            >
                https://chenkecm.github.io/publications/
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                46
            </div>
        
	    <p>
            < 1% match (Internet from 13-Jul-2022) 
        </p>
        
	        <a 
                
                    style="color: #630000" 
                
                href="https://programs.wiki/wiki/yolov5-study-notes.html"
            >
                https://programs.wiki/wiki/yolov5-study-notes.html
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                47
            </div>
        
	    <p>
            < 1% match (Internet from 27-Feb-2022) 
        </p>
        
	        <a 
                
                    style="color: #0270B6" 
                
                href="http://www.yndxxb.ynu.edu.cn/yndxxbzrkxb/article/doi/10.7540/j.ynu.20170581"
            >
                http://www.yndxxb.ynu.edu.cn/yndxxbzrkxb/article/doi/10.7540/j.ynu.20170581
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                48
            </div>
        
	    <p>
            < 1% match ("Computer Vision - ACCV 2014 Workshops", Springer Nature, 2015) 
        </p>
        
	        <a 
                
                    style="color: #330099" 
                
                href="https://doi.org/10.1007/978-3-319-16628-5"
            >
                "Computer Vision - ACCV 2014 Workshops", Springer Nature, 2015
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                49
            </div>
        
	    <p>
            < 1% match ("MultiMedia Modeling", Springer Science and Business Media LLC, 2018) 
        </p>
        
	        <a 
                
                    style="color: #227967" 
                
                href="https://doi.org/10.1007/978-3-319-73600-6"
            >
                "MultiMedia Modeling", Springer Science and Business Media LLC, 2018
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                50
            </div>
        
	    <p>
            < 1% match ("Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications", Springer Science and Business Media LLC, 2018) 
        </p>
        
	        <a 
                
                    style="color: #CB0099" 
                
                href="https://doi.org/10.1007/978-3-319-75193-1"
            >
                "Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications", Springer Science and Business Media LLC, 2018
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                51
            </div>
        
	    <p>
            < 1% match (Haoxiang Liang, Huansheng Song, Xu Yun, Shijie Sun, Yingxuan Wang, Zhaoyang Zhang. "Traffic incident detection based on a global trajectory spatiotemporal map", Complex & Intelligent Systems, 2021) 
        </p>
        
	        <a 
                
                    style="color: #006331" 
                
                href="https://doi.org/10.1007/s40747-021-00602-8"
            >
                Haoxiang Liang, Huansheng Song, Xu Yun, Shijie Sun, Yingxuan Wang, Zhaoyang Zhang. "Traffic incident detection based on a global trajectory spatiotemporal map", Complex & Intelligent Systems, 2021
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                52
            </div>
        
	    <p>
            < 1% match (Fan Yang, Deming Yang, Zhiming He, Yuanhua Fu, Kui Jiang. "Automobile Fine-Grained Detection Algorithm Based on Multi-Improved YOLOv3 in Smart Streetlights", Algorithms, 2020) 
        </p>
        
	        <a 
                
                    style="color: #795AB9" 
                
                href="https://doi.org/10.3390/a13050114"
            >
                Fan Yang, Deming Yang, Zhiming He, Yuanhua Fu, Kui Jiang. "Automobile Fine-Grained Detection Algorithm Based on Multi-Improved YOLOv3 in Smart Streetlights", Algorithms, 2020
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                53
            </div>
        
	    <p>
            < 1% match (Jie Li, Lei Huang, Zhiqiang Wei, Wenfeng Zhang, Qibing Qin. "Multi-task learning with deformable convolution", Journal of Visual Communication and Image Representation, 2021) 
        </p>
        
	        <a 
                
                    style="color: #935F32" 
                
                href="https://doi.org/10.1016/j.jvcir.2021.103109"
            >
                Jie Li, Lei Huang, Zhiqiang Wei, Wenfeng Zhang, Qibing Qin. "Multi-task learning with deformable convolution", Journal of Visual Communication and Image Representation, 2021
            </a>
        
	</div>
	<div  >
	    
            <div class="number-l">
                54
            </div>
        
	    <p>
            < 1% match (Shun Luo, Juan Yu, Yunjiang Xi, Xiao Liao. "Aircraft Target Detection in Remote Sensing Images Based on Improved YOLOv5", IEEE Access, 2022) 
        </p>
        
	        <a 
                
                    style="color: #ce0031" 
                
                href="https://doi.org/10.1109/ACCESS.2022.3140876"
            >
                Shun Luo, Juan Yu, Yunjiang Xi, Xiao Liao. "Aircraft Target Detection in Remote Sensing Images Based on Improved YOLOv5", IEEE Access, 2022
            </a>
        
	</div>
	<div  id="last" >
	    
            <div class="number-l">
                55
            </div>
        
	    <p>
            < 1% match (Internet from 27-Mar-2022) 
        </p>
        
	        <a 
                
                    style="color: #866712" 
                
                href="https://aimspress.com/article/doi/10.3934/mbe.2021189"
            >
                https://aimspress.com/article/doi/10.3934/mbe.2021189
            </a>
        
	</div>

</div>



<div class="divider">paper text:</div>
<div id="body">
<div role="listitem" aria-setsize="134" aria-posinset="27">YOLO Sparse Training and Model Pruning for Street View House Numbers Recognition Ruohao Zhang1,*Yijie Lu2,Zhengfei Song3 1 Department of Software <a href="javascript:openDSC(779983848, 37, '40');" onmouseover="doRollover(27);" onmouseout="undoRollover(27);" id="40" name="27" style="color:#0270B6" class="#0270B6"><span class="b-ref">27</span>Engineering, Nanjing University 2 Department of<span> Artificial Intelligence, </span>Nanjing University 3 School of<span> Electronic and Information </span>Engineering</a></div><div role="listitem" aria-setsize="134" aria-posinset="49">, Tongji University *Corresponding author. Email: 201250203@smail.nju.<a href="javascript:openDSC(584431444, 37, '62');" onmouseover="doRollover(49);" onmouseout="undoRollover(49);" id="62" name="49" style="color:#227967" class="#227967"><span class="b-ref">49</span>edu.cn. ABSTRACT This paper proposes a</a></div><div role="listitem" aria-setsize="134" aria-posinset="3"> YOLO (You Only Look Once) sparse training and model pruning technique for recognizing <a href="javascript:openDSC(3704845654, 3722, '8');" onmouseover="doRollover(3);" onmouseout="undoRollover(3);" id="8" name="3" style="color:blue" class="blue"><span class="b-ref">3</span>house numbers in street view images<span>. YOLO </span>is a</a></div><div role="listitem" aria-setsize="134" aria-posinset="29"> popular object detection algorithm that has <a href="javascript:openDSC(63298063, 3797, '42');" onmouseover="doRollover(29);" onmouseout="undoRollover(29);" id="42" name="29" style="color:#227967" class="#227967"><span class="b-ref">29</span>achieved state-of-the-art performance in various computer vision tasks. However</a></div><div role="listitem" aria-setsize="134" aria-posinset="41">, its large model size and computational complexity limit its deployment on resource-constrained devices <a href="javascript:openDSC(773815035, 37, '54');" onmouseover="doRollover(41);" onmouseout="undoRollover(41);" id="54" name="41" style="color:#D10A0A" class="#D10A0A"><span class="b-ref">41</span>such as smartphones and embedded<span> systems. </span>To address this issue</a></div><div role="listitem" aria-setsize="134" aria-posinset="36">, we use a sparse training technique that trains YOLO with L1 norm regularization to encourage the network to learn sparse representations. This results in a significant reduction in the number of parameters and computation without sacrificing accuracy. Furthermore, we apply a model pruning technique to the sparse-trained model to further reduce the model size and computation.We evaluate our proposed method on the SVHN (Street View House Numbers) dataset and show that it achieves comparable performance to the original YOLO model while reducing the model size by 5% and the computation time by 7%. Overall, our proposed YOLO sparse training and model pruning technique provides an effective solution for deploying YOLO-based object detection models on resource-constrained devices. Keywords: YOLO, sparse training, deep learning, model pruning. <a href="javascript:openDSC(54410415, 37, '49');" onmouseover="doRollover(36);" onmouseout="undoRollover(36);" id="49" name="36" style="color:#63009c" class="#63009c"><span class="b-ref">36</span>1. INTRODUCTION With the<span> rapid </span>development of<span> intelligent </span>technology, we have<span> stepped into </span>the</a></div><div role="listitem" aria-setsize="134" aria-posinset="26"> era of intelligence, where a new intelligent way of life has become the trend of social development and real-time accurate recognition of characters is one of the key technologies. In recent years, Optical Character Recognition is a topic of wide interest <a href="javascript:openDSC(773346423, 37, '39');" onmouseover="doRollover(26);" onmouseout="undoRollover(26);" id="39" name="26" style="color:#630000" class="#630000"><span class="b-ref">26</span>in the field of computer vision and it has become the<span> technical core </span>of</a></div><div role="listitem" aria-setsize="134" aria-posinset="7"> many new applications [1]. However, in real scene images such as the street view house numbers, characters are affected by many external factors, such as illumination, noise, and wear and tear, influencing the accuracy of target positioning and resulting in characters that are difficult to recognize or easily misidentified. Therefore, character recognition in real scenes still needs to be improved and the study of Optical Character Recognition in real scenes is undoubtedly meaningful and important. Existing character recognition techniques <a href="javascript:openDSC(767970443, 37, '16');" onmouseover="doRollover(7);" onmouseout="undoRollover(7);" id="16" name="7" style="color:#0270B6" class="#0270B6"><span class="b-ref">7</span>can be broadly classified into<span> two </span>categories: recognition based on manual feature extraction and recognition based on machine learning</a></div><div role="listitem" aria-setsize="134" aria-posinset="18">. The former mainly uses <a href="javascript:openDSC(1641245580, 3800, '30');" onmouseover="doRollover(18);" onmouseout="undoRollover(18);" id="30" name="18" style="color:#cc0066" class="#cc0066"><span class="b-ref">18</span>feature extraction methods such as SIFT (Scale Invariant Feature Transform) and HOG (Histogram of Oriented</a></div><div role="listitem" aria-setsize="134" aria-posinset="7"> Gradients), combined with classifiers to recognize characters [2][3]. This method is traditional, requires extensive processing of samples and is vulnerable to subjective factors. In contrast, the latter, recognition based on machine learning, <a href="javascript:openDSC(767970443, 37, '17');" onmouseover="doRollover(7);" onmouseout="undoRollover(7);" id="17" name="7" style="color:#0270B6" class="#0270B6"><span class="b-ref">7</span>uses machine learning methods such as<span> CNN (</span>Convolutional Neural Networks</a></div><div role="listitem" aria-setsize="134" aria-posinset="28">) or other networks to directly recognize and classify characters. The recognition effectiveness <a href="javascript:openDSC(774259330, 37, '41');" onmouseover="doRollover(28);" onmouseout="undoRollover(28);" id="41" name="28" style="color:#330099" class="#330099"><span class="b-ref">28</span>of this method is closely related to the size of the<span> sample </span>and</a></div><div role="listitem" aria-setsize="134" aria-posinset="43"> the number of training sessions. As computer vision technology has been developing rapidly and <a href="javascript:openDSC(53191483, 37, '56');" onmouseover="doRollover(43);" onmouseout="undoRollover(43);" id="56" name="43" style="color:blue" class="blue"><span class="b-ref">43</span>the collection of large amounts of data is</a></div><div role="listitem" aria-setsize="134" aria-posinset="33"> becoming more and more easier, automatic feature extraction methods have become a hot topic of research, and researchers pay more attention to find ways to optimize this method to improve recognition efficiency and reduce training time. Due to the continuous research in deep learning, more models with better performance are proposed and put into use. This paper, which is based on a discussion of theoretical knowledge related to deep learning networks, some mainstream models and their performance, focuses on YOLO v5 (You Only Look Once) and explores automatic recognition methods based on YOLO v5. The main research includes: (1) Briefly discuss and summarize the background and significance of this paper, and outline its research status in terms of the current status of recognition of street view house numbers, neural networks and deep learning. (2) Focus on YOLO v5 and the application of YOLO v5 to the street view house numbers recognition as well as using a dataset of street view house numbers images taken from real scenes as objects. (3) Research model optimization methods, experiment with pruning, and apply the pruning algorithms to YOLO v5 to improve the effectiveness of YOLO v5 models for street view house numbers recognition. 2. RELATED WORK In the range of recognizing the street view house numbers recognition, there are several different ways that are worth being mentioned. In the past, people used the classifier to classify the number into different categories and then translated the number string. E. Kussul et al. [4] first introduced the improved version of RSC classifier and perceptron, LIRA(LImited Receptive Area), with a view to ameliorate the classification speed. They deleted the GROUP layer of the RSC classifier. Instead, they replaced each pair neurons in the GROUP layer with just only one neuron which can represent either ON or OFF. Next, people used CNN to make this process much more accurate and efficient. Chen, L. et al. [5] proposed an inventive way to increase the accuracy of CNN classifier, distorting the pictures. Since they thought that random distortion was contradictory with normalization and CNN was a very powerful tool, they chose random distortion to increase the accuracy of their model. J. Redmon [6] proposed YOLO (You Only Look Once) as a tool to recognize the object in 2016, which brought about sensation at that time. YOLO is extremely fast as it predicts the result based on the entire image information. J. Redmon stated that YOLO was good at balancing the accuracy and efficiency. In most cases, the performance of YOLO is far better than all the methods such as Faster R-CNN that belong to the CNN family. Huang, R. [7] also believed that YOLO-LITE, another version of yolo used to work on the devices without GPU, is the best lightweight real-time object detection tool. Furthermore, Cao, L.-C. et al. [8] not only used YOLO to locate the text but also proposed an algorithm to identify whether the text boxes that had been located were in the same line, which contributed to the higher accuracy and efficiency of handwritten text recognition. However, an image segmentation may not be a must for digits recognition. Guo, Q. et al.[9] combined CNN with hidden Markov model(HMM) <a href="javascript:openDSC(3385318037, 3791, '46');" onmouseover="doRollover(33);" onmouseout="undoRollover(33);" id="46" name="33" style="color:#935F32" class="#935F32"><span class="b-ref">33</span>in a hybrid fashion to form the hybrid CNN-HMM architecture</a></div><div role="listitem" aria-setsize="134" aria-posinset="48"><a href="javascript:openDSC(501688272, 37, '61');" onmouseover="doRollover(48);" onmouseout="undoRollover(48);" id="61" name="48" style="color:#330099" class="#330099"><span class="b-ref">48</span>to solve street view number recognition problem</a></div><div role="listitem" aria-setsize="134" aria-posinset="37">. They transformed the problem into sequence recognition under probabilistic processing instead of identifying isolated characters after performing segmentation operations, saving a lot of labor in designing complex algorithms and feature labelling. Also, A.G. Hochuli [10] proposed a way to recognize numbers without segmentation algorithms. They created <a href="javascript:openDSC(586243597, 37, '50');" onmouseover="doRollover(37);" onmouseout="undoRollover(37);" id="50" name="37" style="color:#A85503" class="#A85503"><span class="b-ref">37</span>a framework based on four task-specific classifiers<span>, one for estimating </span>the</a></div><div role="listitem" aria-setsize="134" aria-posinset="34"> number of touching components and three respectively recognizing [0-9],[00-99] and [000-999] numbers. With experiments on two large datasets, their work were proved to achieve SOTA. When researchers used the data in the dataset to train the model, they might find some data might be damaged. When Shi S.-X. et al. [11] used PCA to train the model with some damaged data, they decided to repair the damaged picture. However, the results were not satisfying. As a result, they proposed the hypothesis that deleting these damaged data before training may be better. 3. DATABASE 3.1. <a href="javascript:openDSC(2214984814, 1840, '47');" onmouseover="doRollover(34);" onmouseout="undoRollover(34);" id="47" name="34" style="color:#ce0031" class="#ce0031"><span class="b-ref">34</span>Street View House Numbers Dataset<span> We use </span>the Street View House Numbers<span> Dataset (</span>SVHN</a></div><div role="listitem" aria-setsize="134" aria-posinset="3">) <a href="javascript:openDSC(3704845654, 3722, '9');" onmouseover="doRollover(3);" onmouseout="undoRollover(3);" id="9" name="3" style="color:blue" class="blue"><span class="b-ref">3</span>in Google Street View<span> Image, </span>a real-world image dataset for developing machine learning and object recognition algorithms with minimal data preprocessing and formatting<span> requirements. </span>It can be<span> viewed </span>as<span> stylistically </span>similar to MNIST</a></div><div role="listitem" aria-setsize="134" aria-posinset="8"> dataset. Still, it contains <a href="javascript:openDSC(871413143, 3800, '18');" onmouseover="doRollover(8);" onmouseout="undoRollover(8);" id="18" name="8" style="color:#330099" class="#330099"><span class="b-ref">8</span>an order of magnitude more labeled data (over 600,000 digit images<span>). It </span>comes from a<span> more complex, </span>unsolved real-world problem (Recognizing Digits and<span> Digits </span>in<span> Images of </span>Natural<span> Scenes). The </span>images<span> in </span>the<span> SVHN </span>dataset</a></div><div role="listitem" aria-setsize="134" aria-posinset="21"> are all colour images with a size of 32x32 pixels and contain numbers from 1 to 5. Each number has a corresponding label, representing its value. The SVHN dataset consists of <a href="javascript:openDSC(1000847608, 3799, '33');" onmouseover="doRollover(21);" onmouseout="undoRollover(21);" id="33" name="21" style="color:#D10A0A" class="#D10A0A"><span class="b-ref">21</span>three parts: training set, test set, and validation set. The training set contains</a></div><div role="listitem" aria-setsize="134" aria-posinset="53"> about <a href="javascript:openDSC(709545033, 37, '66');" onmouseover="doRollover(53);" onmouseout="undoRollover(53);" id="66" name="53" style="color:#935F32" class="#935F32"><span class="b-ref">53</span>73,257<span> images, </span>the test set contains</a></div><div role="listitem" aria-setsize="134" aria-posinset="10"> approximately <a href="javascript:openDSC(1798015187, 3798, '19');" onmouseover="doRollover(10);" onmouseout="undoRollover(10);" id="19" name="10" style="color:#CB0099" class="#CB0099"><span class="b-ref">10</span>26,032 images, and the<span> supplementary </span>set<span> contains about </span>531,131 images<span>. Since </span>the<span> SVHN </span>dataset is</a></div><div role="listitem" aria-setsize="134" aria-posinset="2"> a real-world dataset, it has some unique properties. First, the numbers in the image may have different scales, rotations, distortions, and occlusions, which is a challenge for <a href="javascript:openDSC(773532851, 37, '3');" onmouseover="doRollover(2);" onmouseout="undoRollover(2);" id="3" name="2" style="color:#287B28" class="#287B28"><span class="b-ref">2</span>the training and testing of<span> the number recognition </span>model<span>. Second, </span>the<span> distribution </span>of</a></div><div role="listitem" aria-setsize="134" aria-posinset="38"> digits in the SVHN dataset needs to be more balanced. Some digits appear more frequently than others, which may cause digit recognition models to perform poorly on some numbers. Therefore, when using the SVHN dataset for digit recognition tasks, some special pre-processing and data augmentation operations are required <a href="javascript:openDSC(785239922, 37, '51');" onmouseover="doRollover(38);" onmouseout="undoRollover(38);" id="51" name="38" style="color:#cc0066" class="#cc0066"><span class="b-ref">38</span>to improve the<span> performance </span>and robustness of the model. The</a></div><div role="listitem" aria-setsize="134" aria-posinset="50"> SVHN <a href="javascript:openDSC(584460196, 37, '63');" onmouseover="doRollover(50);" onmouseout="undoRollover(50);" id="63" name="50" style="color:#CB0099" class="#CB0099"><span class="b-ref">50</span>dataset has been widely used for<span> training and </span>testing</a></div><div role="listitem" aria-setsize="134" aria-posinset="2"> digit recognition models and strongly influences academia and industry. It is a crucial benchmark dataset in digit recognition, which can be <a href="javascript:openDSC(773532851, 37, '4');" onmouseover="doRollover(2);" onmouseout="undoRollover(2);" id="4" name="2" style="color:#287B28" class="#287B28"><span class="b-ref">2</span>used to evaluate and compare the performance</a></div><div role="listitem" aria-setsize="134" aria-posinset="24"> of different digit recognition models and algorithms. 3.2. Deal with the dataset First, we need to read the JSON file, which contains the digital tag, number box location, and other information corresponding to each picture <a href="javascript:openDSC(583082419, 37, '36');" onmouseover="doRollover(24);" onmouseout="undoRollover(24);" id="36" name="24" style="color:brown" class="brown"><span class="b-ref">24</span>in the training set<span>. Then, we load </span>each image</a></div><div role="listitem" aria-setsize="134" aria-posinset="24"> and get <a href="javascript:openDSC(583082419, 37, '37');" onmouseover="doRollover(24);" onmouseout="undoRollover(24);" id="37" name="24" style="color:brown" class="brown"><span class="b-ref">24</span>the width and height of<span> the loaded </span>image</a></div><div role="listitem" aria-setsize="134" aria-posinset="25"> for subsequent normalization operations. Next, we get each number's label and location information in each picture. After that, we normalize <a href="javascript:openDSC(743800684, 37, '38');" onmouseover="doRollover(25);" onmouseout="undoRollover(25);" id="38" name="25" style="color:#B64B01" class="#B64B01"><span class="b-ref">25</span>the position<span> information </span>of the<span> number </span>box<span>, that is, </span>the coordinates<span> and width </span>of the upper left corner</a></div><div role="listitem" aria-setsize="134" aria-posinset="51"> of each numeric box divided by <a href="javascript:openDSC(763528542, 37, '64');" onmouseover="doRollover(51);" onmouseout="undoRollover(51);" id="64" name="51" style="color:#006331" class="#006331"><span class="b-ref">51</span>the width and height of the image</a></div><div role="listitem" aria-setsize="134" aria-posinset="2"> for subsequent model training. In the end, we write the processed digital label and normalized number box location information to a txt file named after the image file. This process can help us convert the dataset to the proper format for the Yolov5 model. 4. YOLOV5 MODEL Figure 1 Structure of YOLO v5 Figure 2 Structure of small parts in YOLO v5 The input side of Yolov5 adopts <a href="javascript:openDSC(773532851, 37, '5');" onmouseover="doRollover(2);" onmouseout="undoRollover(2);" id="5" name="2" style="color:#287B28" class="#287B28"><span class="b-ref">2</span>Mosaic data enhancement<span>, which </span>uses 4 images, randomly<span> scaled, cropped </span>and</a></div><div role="listitem" aria-setsize="134" aria-posinset="2"> arranged to stitch. <a href="javascript:openDSC(773532851, 37, '6');" onmouseover="doRollover(2);" onmouseout="undoRollover(2);" id="6" name="2" style="color:#287B28" class="#287B28"><span class="b-ref">2</span>It solves the problem of<span> large number of </span>small</a></div><div role="listitem" aria-setsize="134" aria-posinset="6"> targets and unbalanced ratio of small to medium-sized targets in the dataset, helping to enrich the dataset and reduce the GPU at the same time. The input side also performs adaptive anchor frame calculation. During the <a href="javascript:openDSC(717586958, 2, '15');" onmouseover="doRollover(6);" onmouseout="undoRollover(6);" id="15" name="6" style="color:#630000" class="#630000"><span class="b-ref">6</span>network training, the network outputs<span> the predicted </span>frame on the<span> basis of the </span>initial anchor frame, then compares it with the<span> real </span>frame to calculate the difference between the two, and iterates the network parameters by<span> backward </span>updating. In</a></div><div role="listitem" aria-setsize="134" aria-posinset="54"> addition, <a href="javascript:openDSC(752908269, 37, '67');" onmouseover="doRollover(54);" onmouseout="undoRollover(54);" id="67" name="54" style="color:#ce0031" class="#ce0031"><span class="b-ref">54</span>adaptive image scaling is<span> also performed </span>to<span> uniformly </span>scale</a></div><div role="listitem" aria-setsize="134" aria-posinset="40"> images of different lengths and widths to the same <a href="javascript:openDSC(740418074, 37, '53');" onmouseover="doRollover(40);" onmouseout="undoRollover(40);" id="53" name="40" style="color:#336699" class="#336699"><span class="b-ref">40</span>standard size before sending them<span> to </span>the detection network. The</a></div><div role="listitem" aria-setsize="134" aria-posinset="32"> Backbone is firstly sliced in <a href="javascript:openDSC(740734666, 37, '45');" onmouseover="doRollover(32);" onmouseout="undoRollover(32);" id="45" name="32" style="color:#795AB9" class="#795AB9"><span class="b-ref">32</span>the Focus structure<span>, where </span>the original 608*608*3 image is<span> sliced </span>into</a></div><div role="listitem" aria-setsize="134" aria-posinset="19"> 304*304*12 characteristic pattern, and then convolved with 32 convolution kernels once, and finally into 304*304*32 characteristic pattern. Subsequently, the CSP structure CSP1_X is built based on the CSPNet (Cross Stage Paritial Network), which enables the convolutional neural network to maintain accuracy and reduce computation and memory costs while keeping lightweight. The Neck part <a href="javascript:openDSC(2329125753, 1, '31');" onmouseover="doRollover(19);" onmouseout="undoRollover(19);" id="31" name="19" style="color:#21785B" class="#21785B"><span class="b-ref">19</span>adopts the structure of FPN+PAN<span>, where the </span>FPN<span> layer </span>conveys strong semantic features from top<span> down, and a </span>bottom</a></div><div role="listitem" aria-setsize="134" aria-posinset="46">-<a href="javascript:openDSC(3137963423, 3798, '59');" onmouseover="doRollover(46);" onmouseout="undoRollover(46);" id="59" name="46" style="color:#630000" class="#630000"><span class="b-ref">46</span>up feature pyramid<span> is added after </span>the FPN layer<span> to convey </span>strong<span> localization </span>features</a></div><div role="listitem" aria-setsize="134" aria-posinset="13"> upward. Also this part adds the CSP2 structure <a href="javascript:openDSC(700487233, 2, '23');" onmouseover="doRollover(13);" onmouseout="undoRollover(13);" id="23" name="13" style="color:#935F32" class="#935F32"><span class="b-ref">13</span>to enhance the network feature fusion. The<span> Prediction part </span>uses<span> CIOU_</span>Loss as the loss function of the Bounding box</a></div><div role="listitem" aria-setsize="134" aria-posinset="39">. <a href="javascript:openDSC(777777258, 37, '52');" onmouseover="doRollover(39);" onmouseout="undoRollover(39);" id="52" name="39" style="color:#21785B" class="#21785B"><span class="b-ref">39</span>CIOU_Loss = 1 − CIOU (1) Distance_22<span> 𝑣 </span>2<span> CIOU = </span>IOU</a></div><div role="listitem" aria-setsize="134" aria-posinset="20"> − Distance_C 2 − (1 − I0U) + 𝑣 (2) area of intersection of prediction frame and target frame 𝐼𝑁𝑈 = area of concatenation <a href="javascript:openDSC(2258391872, 1, '32');" onmouseover="doRollover(20);" onmouseout="undoRollover(20);" id="32" name="20" style="color:#336699" class="#336699"><span class="b-ref">20</span>of prediction frame and<span> target </span>frame<span> (3) </span>v is a parameter<span> that measures </span>the consistency of<span> the </span>aspect ratio</a></div><div role="listitem" aria-setsize="134" aria-posinset="12">， 4 wgt wp 𝜈 = 𝜋2 (arctan hgt − arctan hp ) (4) <a href="javascript:openDSC(786104853, 37, '22');" onmouseover="doRollover(12);" onmouseout="undoRollover(12);" id="22" name="12" style="color:#795AB9" class="#795AB9"><span class="b-ref">12</span>Distance_C is the diagonal distance of the minimum outer rectangle<span> of the prediction </span>frame and the<span> target frame, and </span>Distance<span>_2 is </span>the<span> Euclidean distance </span>of the two</a></div><div role="listitem" aria-setsize="134" aria-posinset="16"> centroids. CIOU_Loss takes <a href="javascript:openDSC(2158712838, 1, '27');" onmouseover="doRollover(16);" onmouseout="undoRollover(16);" id="27" name="16" style="color:#63009c" class="#63009c"><span class="b-ref">16</span>the overlap area, centroid distance, and aspect ratio<span> into account, making </span>the prediction frame regression faster and more accurate</a></div><div role="listitem" aria-setsize="134" aria-posinset="52">. The nms non-maximum suppression operation is subsequently performed for many target frame filters. 5. PRUNE 5.1. Sparse Training When building neural networks, the huge number of parameters can significantly slow down the training process. To speed up training and deployment, we implemented sparse learning techniques to compress our model and accelerate training and inference. The core idea of sparse training is setting some neuron weight values to zero, making the model sparser. This decreases the number of parameters and calculations needed, reducing storage requirements. Specifically, we applied an L1 norm regularization approach to constrain weight values. We also pruned unimportant channels to further minimize parameters. By implementing these techniques, we were able to optimize our model size, training time, and inference speed. Sparse training and channel pruning are useful tools for efficiently constructing and deploying deep neural networks. Streamlining networks in this way makes them more practical for real-world use. By trimming away unnecessary parameters and channels, we achieved a model that trains and infers quickly without sacrificing performance. Sparse training methods thus facilitate the development of deep learning models that strike a good balance between accuracy and efficiency. 5.2. BN Pruning Batch Normalization (BN) is a normalization technology commonly used in deep learning, which can accelerate convergence and improve <a href="javascript:openDSC(782948831, 37, '65');" onmouseover="doRollover(52);" onmouseout="undoRollover(52);" id="65" name="52" style="color:#795AB9" class="#795AB9"><span class="b-ref">52</span>the model's generalization<span> performance. However, as </span>the model</a></div><div role="listitem" aria-setsize="134" aria-posinset="2"> size increases, the calculation amount of the BN layer also increases, and the storage and reasoning cost <a href="javascript:openDSC(773532851, 37, '7');" onmouseover="doRollover(2);" onmouseout="undoRollover(2);" id="7" name="2" style="color:#287B28" class="#287B28"><span class="b-ref">2</span>of the model<span> also increases. </span>In order to<span> reduce </span>the<span> size </span>of<span> the </span>model and</a></div><div role="listitem" aria-setsize="134" aria-posinset="4"> accelerate model reasoning, BN-layer pruning has become a practical technology. The principle of BN-layer pruning is to find channels or neurons that have less impact on the model's accuracy by analyzing the statistical information of the BN layer and deleting them from the network. The importance of each channel or neuron in <a href="javascript:openDSC(665387882, 37, '10');" onmouseover="doRollover(4);" onmouseout="undoRollover(4);" id="10" name="4" style="color:brown" class="brown"><span class="b-ref">4</span>the BN layer is<span> obtained by analyzing </span>the mean and standard deviation</a></div><div role="listitem" aria-setsize="134" aria-posinset="4"><a href="javascript:openDSC(665387882, 37, '11');" onmouseover="doRollover(4);" onmouseout="undoRollover(4);" id="11" name="4" style="color:brown" class="brown"><span class="b-ref">4</span>of the BN layer<span>; that </span>is, the mean and standard deviation of</a></div><div role="listitem" aria-setsize="134" aria-posinset="4"> the BN layer are obtained through <a href="javascript:openDSC(665387882, 37, '12');" onmouseover="doRollover(4);" onmouseout="undoRollover(4);" id="12" name="4" style="color:brown" class="brown"><span class="b-ref">4</span>the forward propagation of<span> the </span>training<span> set, and then </span>the</a></div><div role="listitem" aria-setsize="134" aria-posinset="15"> importance of each channel or neuron is determined according to the size of <a href="javascript:openDSC(716757119, 37, '25');" onmouseover="doRollover(15);" onmouseout="undoRollover(15);" id="25" name="15" style="color:#866712" class="#866712"><span class="b-ref">15</span>the mean and standard deviation. The<span> pruning of the </span>BN layer</a></div><div role="listitem" aria-setsize="134" aria-posinset="30"><a href="javascript:openDSC(2106072814, 3799, '43');" onmouseover="doRollover(30);" onmouseout="undoRollover(30);" id="43" name="30" style="color:#CB0099" class="#CB0099"><span class="b-ref">30</span>can be divided into the following steps<span>: 1. Calculate </span>the<span> importance </span>of each<span> channel or neuron </span>in the</a></div><div role="listitem" aria-setsize="134" aria-posinset="4"> BN layer: Calculate the significance of each channel or neuron <a href="javascript:openDSC(665387882, 37, '13');" onmouseover="doRollover(4);" onmouseout="undoRollover(4);" id="13" name="4" style="color:brown" class="brown"><span class="b-ref">4</span>according to the mean and standard deviation<span> of </span>the BN layer<span> of the </span>training</a></div><div role="listitem" aria-setsize="134" aria-posinset="15"> set, and then sort it according to the importance. 2. Delete low-importance channels or neurons: Remove less important channels or neurons from the BN layer and delete the related weights accordingly. 3. Fixed pruning model: After deleting channels or neurons, the remaining models need to be fine-tuned to restore accuracy, and the model needs to be compressed and accelerated. There are many ways to prune the BN layer, the most common of which is to prune based on <a href="javascript:openDSC(716757119, 37, '26');" onmouseover="doRollover(15);" onmouseout="undoRollover(15);" id="26" name="15" style="color:#866712" class="#866712"><span class="b-ref">15</span>the mean and standard deviation<span> of </span>the BN layer<span> and </span>to</a></div><div role="listitem" aria-setsize="134" aria-posinset="11"> delete low-importance channels or neurons in the BN layer through thresholds. In addition, there is a BN layer pruning method based on sparse matrix and binarization, which can further reduce the model's storage and calculation and improve the model's running speed. In our program, we use the method based on gradient information to assess the importance of the convolutional nucleus and then selectively prune the convolutional nucleus according to the assessment results. In our experiment, the gradient information of all convolutional nuclei is obtained by training the complete Yolov5 model on the training set. Then, the importance is assessed by calculating the average gradient size of each convolutional kernel. The smaller the average gradient, the smaller the impact of the convolutional kernel on the model; it can be pruned. Then, Yolov5prune uses a method based on L1 regularization to prune the convolutional nucleus selectively, that is, the convolutional nucleus with a gradient value less than a certain threshold, while retaining an essential convolutional nucleus so that the model accuracy loss after pruning is less. 6. EXPERIMENTS AND DISCUSSION The model we used to detect digits was trained on a GeForce RTX 2070 SUPER. We used pytorch 1.21.1+cu116 and Cuda 11.6. The main indicators we used to evaluate our model were Intersection over union(IOU), <a href="javascript:openDSC(2462056692, 3800, '20');" onmouseover="doRollover(11);" onmouseout="undoRollover(11);" id="20" name="11" style="color:#006331" class="#006331"><span class="b-ref">11</span>Precision(P), Recall(R) and mean<span> of </span>Average Precision(mAP). Precision<span> illustrates how accurate </span>the</a></div><div role="listitem" aria-setsize="134" aria-posinset="42"> model is. Recall illustrates how the model can make correct predictions among all ground truth positive examples. Mean of Average Precision stands for the overall accuracy <a href="javascript:openDSC(710368555, 37, '55');" onmouseover="doRollover(42);" onmouseout="undoRollover(42);" id="55" name="42" style="color:#287B28" class="#287B28"><span class="b-ref">42</span>of the model. TP<span> 𝑁 = </span>TP + FP<span> × 100% </span>TP 𝑅 = TP</a></div><div role="listitem" aria-setsize="134" aria-posinset="17"> + FN × 100% 1 AP = ∫ 𝑁(𝑅)d𝑅 0 ∑𝑁𝑖=1 AP𝑖 mAP = 𝑁 (5) (6) (7) (8) In our experiment we trained three models in all, the original one(fig.3), the one used sparse train(fig.4) and the pruned one(fig.5). For each model, we collected how their loss and performance changed by each epoch and is shown below. Figure 3 Results of original model Figure 4 Results of model after sparse training Figure 5 Results of pruned model First, we used Yolov5 directly to train the model. From fig.1, we found that the loss on training set decrease rapidly at the beginning epochs of training and maintain a slower decreasing trend in later epochs. Also, Precision and Recall reached their peak at no longer than 25 epochs. It shows that Yolov5 is capable of dealing with digits recognition problems. The training time of each epoch in Yolov5 took about 12 minutes on a GeForce RTX 2070s and the reasoning time for each image was about 2.8ms which had an FPS at 357/s. The mAP@0.5 is about 0.82. Secondly, we implemented sparse training on Yolov5. From fig.2, we found that the loss kept decreasing until epoch 15 and had a reverse. After about 20 epochs of slow increase, there appeared another reverse and the loss on training set began to decrease again. This was because the training was finished at about epoch 15 and the later epochs led to a overfitting. The training time of each epoch in Yolov5 took about 14 minutes on a GeForce RTX 2070s and the reasoning time for each image was about 3.0ms which had an FPS at 333/s. The mAP@0.5 is about 0.81. Lastly, we pruned the sparse trained model and implemented the result on Yolov5. From fig.3, we found that the model had a higher speed of convergence. Precision and Recall reached peak at about epoch 25. The training time of each epoch in Yolov5 took about 8 minutes on a GeForce RTX 2070s and the reasoning time for each image was about 2.6ms which had an FPS at 384/s. It showed that the pruned model required a much less training time than the original Yolov5 while it remained almost the same performance as the original one. Also, the pruned model had an increase in the speed of reasoning. The mAP@0.5 is about 0.80. In summary, by using efficient training techniques like L1 regularization and channel pruning, we successfully optimized the Yolov5 model to reduce its size, speed up training, and increase inference speed. Our experiments show that the optimized Yolov5 model is 9% smaller than the original, with only a 2 percent drop in mAP. At the same time, training speed increased by 33% and inference speed increased by 7%. It's clear that these sparse training methods achieved great results in optimizing deep learning models. Figure 6 Labels of street view pictures with house numbers Figure 7 Prediction of street view house numbers 7. CONCLUSION We implemented sparce training and pruning on Yolov5 to help improve model performance in recognizing digits in real world enviornments. With our experiments, the use of sparce training and pruning remarkbly reduce the training time required and speed up reasoning. Also it takes a smaller space to store the model while has little loss in its performance. Through our approach, we improved Yolov5 and successfully implement Yolov5 in real world digits recognition. AUTHORS’ CONTRIBUTIONS（字体Times New Roman，字号11.5，<a href="javascript:openDSC(973577087, 772, '28');" onmouseover="doRollover(17);" onmouseout="undoRollover(17);" id="28" name="17" style="color:#A85503" class="#A85503"><span class="b-ref">17</span>加粗，段前12磅，段后8磅</a></div><div role="listitem" aria-setsize="134" aria-posinset="1">） <a href="javascript:openDSC(2866435356, 3796, '1');" onmouseover="doRollover(1);" onmouseout="undoRollover(1);" id="1" name="1" style="color:#D10A0A" class="#D10A0A"><span class="b-ref">1</span>The title<span> "AUTHORS’ CONTRIBUTIONS" </span>should be in all caps. ACKNOWLEDGMENTS</a></div><div role="listitem" aria-setsize="134" aria-posinset="17">（字体Times New Roman，字号11.5，<a href="javascript:openDSC(973577087, 772, '29');" onmouseover="doRollover(17);" onmouseout="undoRollover(17);" id="29" name="17" style="color:#A85503" class="#A85503"><span class="b-ref">17</span>加粗，段前12磅，段后8磅</a></div><div role="listitem" aria-setsize="134" aria-posinset="1">） <a href="javascript:openDSC(2866435356, 3796, '2');" onmouseover="doRollover(1);" onmouseout="undoRollover(1);" id="2" name="1" style="color:#D10A0A" class="#D10A0A"><span class="b-ref">1</span>The title<span> "ACKNOWLEDGMENTS" </span>should be in all caps and should be placed above the references. The references should be consistent within the article and follow the same style. List all the references with full details. REFERENCES [1</a></div><div role="listitem" aria-setsize="134" aria-posinset="23">]王泰刚.<a href="javascript:openDSC(414163183, 1274, '35');" onmouseover="doRollover(23);" onmouseout="undoRollover(23);" id="35" name="23" style="color:blue" class="blue"><span class="b-ref">23</span>基于神经网络的手写字符识别</a></div><div role="listitem" aria-setsize="134" aria-posinset="9">系统.2013.大连海事大学,MA thesis. [2] David G. Lowe."<a href="javascript:openDSC(585653554, 3793, '21');" onmouseover="doRollover(9);" onmouseout="undoRollover(9);" id="21" name="9" style="color:#227967" class="#227967"><span class="b-ref">9</span>Distinctive Image Features from Scale-Invariant Keypoints.." International Journal of Computer Vision 60<span>.2(2004). </span>doi:10.1023/B:VISI.0000029664.99615.94</a></div><div role="listitem" aria-setsize="134" aria-posinset="22">. [<a href="javascript:openDSC(397721008, 3800, '34');" onmouseover="doRollover(22);" onmouseout="undoRollover(22);" id="34" name="22" style="color:#287B28" class="#287B28"><span class="b-ref">22</span>3] Dalal N., Triggs B<span>. "Histograms of oriented gradients for human detection". 2005 </span>IEEE Conference on Computer Vision and Pattern Recognition</a></div><div role="listitem" aria-setsize="134" aria-posinset="14">.IEEE,2005 [4] <a href="javascript:openDSC(606847673, 2, '24');" onmouseover="doRollover(14);" onmouseout="undoRollover(14);" id="24" name="14" style="color:#ce0031" class="#ce0031"><span class="b-ref">14</span>Kussul, Ernst, and Tatiana Baidyk<span>. "Improved method of handwritten digit recognition tested on MNIST database." </span>Image and Vision Computing 22.12 (2004): 971-981. [5<span>] Chen, Li, </span>et al</a></div><div role="listitem" aria-setsize="134" aria-posinset="35">. "Beyond human recognition: A CNN-based framework for handwritten character recognition." <a href="javascript:openDSC(3428757541, 3791, '48');" onmouseover="doRollover(35);" onmouseout="undoRollover(35);" id="48" name="35" style="color:#866712" class="#866712"><span class="b-ref">35</span>2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR). IEEE, 2015</a></div><div role="listitem" aria-setsize="134" aria-posinset="5">. [6] <a href="javascript:openDSC(2378691055, 3800, '14');" onmouseover="doRollover(5);" onmouseout="undoRollover(5);" id="14" name="5" style="color:#B64B01" class="#B64B01"><span class="b-ref">5</span>Redmon, Joseph, et al<span>. "You only look once: Unified, real-time object detection." </span>Proceedings of the IEEE conference on computer vision and pattern recognition. 2016<span>. [7] </span>Huang, Rachel, Jonathan Pedoeem, and Cuixian Chen<span>. "YOLO-LITE: a real-time object detection algorithm optimized for non-GPU computers." </span>2018 IEEE international conference on big data (big data). IEEE, 2018<span>. [8] Cao, Langcai, </span>et al</a></div><div role="listitem" aria-setsize="134" aria-posinset="55">. "A text detection algorithm for image of student exercises based on CTPN and enhanced YOLOv3." <a href="javascript:openDSC(853434263, 3798, '68');" onmouseover="doRollover(55);" onmouseout="undoRollover(55);" id="68" name="55" style="color:#866712" class="#866712"><span class="b-ref">55</span>IEEE Access 8 (2020): 176924-176934</a></div><div role="listitem" aria-setsize="134" aria-posinset="47">. [9] Guo Qiang, <a href="javascript:openDSC(3251041569, 3797, '60');" onmouseover="doRollover(47);" onmouseout="undoRollover(47);" id="60" name="47" style="color:#0270B6" class="#0270B6"><span class="b-ref">47</span>et al<span>. “Hybrid CNN-HMM Model for Street View House Number Recognition.” </span>Asian Conference on Computer Vision.2014</a></div><div role="listitem" aria-setsize="134" aria-posinset="31">. [10] <a href="javascript:openDSC(647328011, 37, '44');" onmouseover="doRollover(31);" onmouseout="undoRollover(31);" id="44" name="31" style="color:#006331" class="#006331"><span class="b-ref">31</span>Hochuli, Andre G., et al<span>. "Handwritten digit segmentation: Is it still necessary?." </span>Pattern Recognition 78 (2018): 1-11</a></div><div role="listitem" aria-setsize="134" aria-posinset="45">. [11]史素霞,常婉秋,and宋志英."基于UCI数据集的OCR光学字符识别."<a href="javascript:openDSC(3797405149, 3799, '58');" onmouseover="doRollover(45);" onmouseout="undoRollover(45);" id="58" name="45" style="color:#B64B01" class="#B64B01"><span class="b-ref">45</span>科技创新与应用12</a></div><div role="listitem" aria-setsize="134" aria-posinset="44">.35(2022):50-53. <a href="javascript:openDSC(2854834240, 3800, '57');" onmouseover="doRollover(44);" onmouseout="undoRollover(44);" id="57" name="44" style="color:brown" class="brown"><span class="b-ref">44</span>doi:10.19981/j.CN23-1581/G3.2022</a></div><div role="listitem" aria-setsize="134" aria-posinset="">.35.012. </div>
</div>

</body>
</html>

